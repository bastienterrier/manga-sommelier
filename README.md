# Manga Sommelier üç∑

Votre prochain coup de c≈ìur manga, d√©nich√© par une Intelligence Artificielle qui analyse vos go√ªts uniques.

[Image de Screenshot de l'application Manga Sommelier]
![Homepage de l'application Manga Sommelier](./docs/assets/homepage.png)
![Suggestions de l'application Manga Sommelier](./docs/assets/suggestions.png)

## Le Concept

Fatigu√© des recommandations g√©n√©riques ? **Manga Sommelier** est une application web exp√©rimentale qui vise √† offrir des suggestions de lecture ultra-personnalis√©es.

Contrairement aux algorithmes classiques qui se basent sur "les utilisateurs qui ont aim√© X ont aussi aim√© Y", notre approche est diff√©rente :

1.  Vous nous dites pr√©cis√©ment ce que vous avez lu et ce que vous avez **ador√©, aim√©, ou pas aim√©**.
2.  Nous traduisons cette s√©lection en une instruction nuanc√©e pour un **LLM (Large Language Model)** qui tourne en local.
3.  L'IA analyse vos go√ªts en profondeur et g√©n√®re 5 recommandations sur mesure, avec une explication de pourquoi chaque manga pourrait vous plaire.

## ‚ú® Technologies Utilis√©es

* **Frontend :** Next.js, React, TypeScript, Tailwind CSS
* **Donn√©es Manga :** [Jikan API (pour MyAnimeList)](https://jikan.moe/)
* **Moteur de Recommandation :** [Ollama](https://ollama.com/) avec un mod√®le customis√© (bas√© sur Mistral).

## ‚ö†Ô∏è Informations Importantes sur le Projet

Ce projet est une exp√©rimentation personnelle, d√©velopp√©e sur mon temps libre. Merci de prendre en compte les points suivants :

### D√©pendance √† Jikan API

Toutes les donn√©es concernant les mangas (titres, couvertures, synopsis) proviennent de l'excellente mais non-officielle **Jikan API**. Ce service est **gratuit et public**, ce qui implique des **quotas d'appels limit√©s** pour √©viter les abus.

‚û°Ô∏è **Cons√©quence :** Il est possible que la recherche de mangas soit parfois lente ou temporairement indisponible si le quota est atteint. **Merci pour votre indulgence si le service rencontre des difficult√©s.**

### Moteur de Recommandation (LLM)

Le c≈ìur de l'application est un LLM qui tourne localement.
La qualit√© des recommandations est directement li√©e √† la performance du mod√®le et √† la pertinence du "prompt" qui lui est envoy√©.

‚û°Ô∏è **Cons√©quence :** Le mod√®le utilis√© est encore **instable et en cours d'optimisation**. Les suggestions peuvent parfois √™tre surprenantes, imparfaites, ou m√™me un peu √† c√¥t√© de la plaque. C'est tout l'enjeu du projet !

## üöÄ Futur du Projet

Ce projet est en constante √©volution. Les prochaines √©tapes pr√©vues sont :

* Am√©lioration continue de la logique de "prompt engineering".
* Test avec diff√©rents mod√®les de LLM pour comparer les r√©sultats.
* Mise √† disposition d'un environement Docker.

## üíª D√©veloppement Local

Pour lancer le projet sur votre machine :

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone https://github.com/bastienterrier/manga-sommelier
    ```
2.  **Installez les d√©pendances :**
    ```bash
    cd [NOM_DU_DOSSIER]
    npm install
    ```
3.  **Lancez le serveur de d√©veloppement :**
    ```bash
    npm run dev
    ```
4.  Assurez-vous d'avoir une instance d'Ollama fonctionnelle qui sert votre mod√®le customis√©.

---

N'h√©sitez pas √† ouvrir une *issue* si vous rencontrez un bug ou si vous avez des suggestions !
